{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaeSaeeda/ComputerScience-2022-2025/blob/main/data_final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbQSYb1HVHvj"
      },
      "outputs": [],
      "source": [
        "# Nessasary imports for the project\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import csv\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "!pip install pandas requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1JjMy1dXqq7",
        "outputId": "7277f5d6-b5ca-4331-9a52-b3a8a2ab8c63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kdramas information saved to 'kdramas_info.csv'\n"
          ]
        }
      ],
      "source": [
        "#creates the primary table for the project, going through each link and adding it to the series column\n",
        "url = \"https://en.wikipedia.org/wiki/Korean_drama\"\n",
        "response = requests.get(url)\n",
        "#checks if information can be scraped from the website\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    table = soup.find_all('table', {'class': 'wikitable'})[2]\n",
        "\n",
        "    if table:\n",
        "        df = pd.read_html(str(table), header=0)[0]\n",
        "        df['Network'].fillna(method='ffill', inplace=True)\n",
        "        df = df.drop(columns=['Ref'])\n",
        "        df.to_csv('kdramas_info.csv', index=False, encoding='utf-8')\n",
        "\n",
        "        print(\"Kdramas information saved to 'kdramas_info.csv'\")\n",
        "    else:\n",
        "        print(\"Table not found on the page.\")\n",
        "else:\n",
        "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siBczvTP7m52",
        "outputId": "25bbc7aa-141b-4a5c-dbec-c0f7313eb316"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kdramas information saved to 'kdramas_info.csv'\n"
          ]
        }
      ],
      "source": [
        "# Adding the column called drama link which is the hyperlink of the drama\n",
        "url = \"https://en.wikipedia.org/wiki/Korean_drama\"\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    table = soup.find_all('table', {'class': 'wikitable'})[2]\n",
        "    if table:\n",
        "        df = pd.read_html(str(table), header=0)[0]\n",
        "        df['Network'].fillna(method='ffill', inplace=True)\n",
        "        drama_links = []\n",
        "        for cell in table.select('tr td:nth-of-type(2)'):\n",
        "            link = cell.find('a')\n",
        "            drama_links.append(link['href'] if link else None)\n",
        "        df['Drama Link'] = drama_links\n",
        "        df = df.drop(columns=['Ref'])\n",
        "\n",
        "        df.to_csv('kdramas_info.csv', index=False, encoding='utf-8')\n",
        "\n",
        "        print(\"Kdramas information saved to 'kdramas_info.csv'\")\n",
        "    else:\n",
        "        print(\"Table not found on the page.\")\n",
        "else:\n",
        "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k23cEftrYyPv",
        "outputId": "12281edf-6ff4-4ea5-fcb4-d6013633bad8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kdramas information with cast details saved to 'kdramas_info_with_cast.csv'\n"
          ]
        }
      ],
      "source": [
        "# Gets the cast members of the kdrama, adds them to the previously created csv\n",
        "def get_cast_info(series_url):\n",
        "    response = requests.get(series_url)\n",
        "    if response.status_code == 200:\n",
        "        series_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        cast_heading = series_soup.find('span', {'id': 'Cast'})\n",
        "        cast_text = ''\n",
        "\n",
        "        if cast_heading:\n",
        "            parent_section = cast_heading.find_parent()\n",
        "            paragraphs = []\n",
        "            current_element = parent_section.find_next()\n",
        "\n",
        "            while current_element and not current_element.name.startswith('h2'):\n",
        "                if current_element.name in ['p', 'ul', 'ol', 'dl']:\n",
        "                    paragraphs.append(current_element.get_text(strip=True))\n",
        "                current_element = current_element.find_next()\n",
        "            cast_text = '\\n'.join(paragraphs)\n",
        "\n",
        "        return cast_text.strip()\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to retrieve the series page. Status code: {response.status_code}\")\n",
        "        return None\n",
        "df = pd.read_csv('kdramas_info.csv')\n",
        "df['Series Link'] = df.apply(lambda row: f\"https://en.wikipedia.org{row['Drama Link']}\" if pd.notnull(row['Drama Link']) and row['Drama Link'].startswith('/wiki/') else None, axis=1)\n",
        "df['Cast'] = ''\n",
        "for index, row in df.iterrows():\n",
        "    series_url = row['Series Link']\n",
        "    if pd.notnull(series_url):\n",
        "        cast_info = get_cast_info(series_url)\n",
        "        if cast_info is not None:\n",
        "            df.at[index, 'Cast'] = cast_info if cast_info else 'N/A'\n",
        "df.to_csv('kdramas_info_with_cast.csv', index=False, encoding='utf-8')\n",
        "print(\"Kdramas information with cast details saved to 'kdramas_info_with_cast.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fy2rC8U_zZq",
        "outputId": "377d4892-e4ed-4033-c12a-9b3a227863f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kdramas cast and synopsis information saved to 'kdramas_info_with_cast_and_synopsis.csv'\n"
          ]
        }
      ],
      "source": [
        "# Adds the synopsis and plot to the culumn and adds it to the casts csv\n",
        "def get_synopsis_info(series_url):\n",
        "    response = requests.get(series_url)\n",
        "    if response.status_code == 200:\n",
        "        series_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        synopsis_heading = series_soup.find('span', {'id': 'Synopsis'})\n",
        "        plot_heading = series_soup.find('span', {'id': 'Plot'})\n",
        "\n",
        "        if synopsis_heading:\n",
        "            parent_section = synopsis_heading.find_parent()\n",
        "        elif plot_heading:\n",
        "            parent_section = plot_heading.find_parent()\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        next_h2 = parent_section.find_next('h2')\n",
        "        paragraphs = parent_section.find_all_next(['p', 'ul', 'ol', 'dl', 'h2'])\n",
        "        paragraphs = paragraphs[:paragraphs.index(next_h2)] if next_h2 else paragraphs\n",
        "\n",
        "        synopsis_text = '\\n'.join(paragraph.get_text(strip=True) for paragraph in paragraphs)\n",
        "        return synopsis_text.strip()\n",
        "\n",
        "    print(f\"Failed to retrieve the series page. Status code: {response.status_code}\")\n",
        "    return None\n",
        "\n",
        "df = pd.read_csv('kdramas_info_with_cast.csv')\n",
        "df['Synopsis'] = ''\n",
        "for index, row in df.iterrows():\n",
        "    series_url = row['Series Link']\n",
        "    if pd.notnull(series_url):\n",
        "        synopsis_info = get_synopsis_info(series_url)\n",
        "        if synopsis_info is not None:\n",
        "            df.at[index, 'Synopsis'] = synopsis_info if synopsis_info else 'N/A'\n",
        "\n",
        "df.to_csv('kdramas_info_with_cast_and_synopsis.csv', index=False, encoding='utf-8')\n",
        "print(\"Kdramas cast and synopsis information saved to 'kdramas_info_with_cast_and_synopsis.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X35vvMBkrBia",
        "outputId": "374af49e-77df-4b88-c7d5-fbfbf547d2d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "K-Dramas Information Table:\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Series                             | Cast          | Synopsis      |\n",
            "+====================================+===============+===============+\n",
            "| The World of the Married           | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Reborn Rich                        | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Sky Castle                         | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Crash Landing on You               | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Reply 1988                         | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Guardian: The Lonely and Great God | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Doctor Cha                         | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Mr. Sunshine                       | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Extraordinary Attorney Woo         | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Mr. Queen                          | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Crash Course in Romance            | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Under the Queen's Umbrella         | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Love (ft. Marriage and Divorce) 2  | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Itaewon Class                      | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Agency                             | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Vincenzo                           | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Our Blues                          | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| 100 Days My Prince                 | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Hospital Playlist                  | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Hospital Playlist 2                | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| King the Land                      | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Hometown Cha-Cha-Cha               | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Signal                             | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| The Lady in Dignity                | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| The Good Bad Mother                | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Hotel del Luna                     | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Red Balloon                        | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Twenty-Five Twenty-One             | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Reply 1994                         | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Prison Playbook                    | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Little Women                       | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| The Uncanny Counter                | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| The Crowned Clown                  | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| My Kids Give Me a Headache         | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Jirisan                            | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Mine                               | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Strong Girl Nam-soon               | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Love (ft. Marriage and Divorce) 3  | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Show Window: The Queen's House     | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Encounter                          | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Military Prosecutor Doberman       | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Memories of the Alhambra           | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Another Miss Oh                    | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Bossam: Steal the Fate             | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| The Light in Your Eyes             | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Strong Girl Bong-soon              | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Love (ft. Marriage and Divorce)    | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Alchemy of Souls: Light and Shadow | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Behind Your Touch                  | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n",
            "| Divorce Attorney Shin              | Contains data | Contains data |\n",
            "+------------------------------------+---------------+---------------+\n"
          ]
        }
      ],
      "source": [
        "# Table is created to show the available information\n",
        "\n",
        "df = pd.read_csv('/content/kdramas_info_with_cast_and_synopsis.csv')\n",
        "\n",
        "# Display information about series, cast, and synopsis\n",
        "table_data = []\n",
        "for index, row in df.iterrows():\n",
        "    series_name = row['Series']\n",
        "    cast_info = 'Contains data' if pd.notna(row['Cast']) else 'No data collected'\n",
        "    synopsis_info = 'Contains data' if pd.notna(row['Synopsis']) else 'No data collected'\n",
        "    table_data.append([series_name, cast_info, synopsis_info])\n",
        "\n",
        "headers = ['Series', 'Cast', 'Synopsis']\n",
        "table = tabulate(table_data, headers=headers, tablefmt='grid')\n",
        "\n",
        "print(\"\\nK-Dramas Information Table:\")\n",
        "print(table)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdWdW8Wgvyatrx8vZU+sXm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}